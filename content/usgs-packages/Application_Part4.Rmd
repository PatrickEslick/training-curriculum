---
title: "Application - Part 4: Publish."
date: "9999-04-01"
author: "Lindsay R. Carr"
slug: "app-part4"
image: "img/main/intro-icons-300px/r-logo.png"
output: USGSmarkdowntemplates::hugoTraining
parent: Introduction to USGS R Packages
weight: 2
draft: true
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
library(knitr)

knit_hooks$set(plot=function(x, options) {
  sprintf("<img src='../%s%s-%d.%s'/ title='%s'/>", 
          options$fig.path, options$label, options$fig.cur, options$fig.ext, options$fig.cap)

})

opts_chunk$set(
  echo=TRUE,
  fig.path="static/app-part4/",
  fig.width = 6,
  fig.height = 6,
  fig.cap = "TODO"
)

knit_hooks$set(addToggle = function(before, options, envir) {
    if(before) {
      sprintf('<button class="ToggleButton" onclick="toggle_visibility(\'%1$s\')">Show Answer</button>
              <div id="%1$s" style="display:none">', opts_current$get('label'))
    } else {
      '</div>'
    }
})

set.seed(1)
```

```{r sbtools-auth, echo=FALSE}
# run vizlab::storeSBcreds() once before this can work
credList <- readRDS(file.path(path.expand('~'), ".vizlab/sbCreds"))
un <- rawToChar(credList$username)
pw <- rawToChar(credList$password)
sbtools::authenticate_sb(un, pw)
```

In this section, we will complete the workflow and push the finished plots to ScienceBase from within the script. See the [previous section](/app-part3) to see how we created the plots (or expand the code below).

Getting sites and data:

```{r sites-data-plot, addToggle="ON", echo=FALSE, message=FALSE, warning=FALSE}
library(sbtools)
library(dataRetrieval)
library(geoknife)
library(tidyr)
library(dplyr)

# identify site id and query for files
sb_site_id <- "59848b35e4b0e2f5d46717d1"
avail_files <- item_list_files(sb_site_id)

# use appropriate reader to get file (tab delimited) into R & get site numbers
sb_sites_df <- read.table(avail_files$url[1], sep="\t", header=TRUE,
                          colClasses = "character", stringsAsFactors = FALSE)
sites <- sb_sites_df$site_number

# get HUC 8 codes for precip data
sb_sites_info <- readNWISsite(sites)
huc8s <- sb_sites_info$huc_cd

# define period
startDate <- "2015-10-01"
endDate <- "2016-09-30"

# download nutrient data
pcodes_nitrogen <- c("00613", "00618", "00631")
pcodes_phosphorus <- c("00665")
nitrogen_data <- readNWISqw(siteNumbers = sites, parameterCd = pcodes_nitrogen,
                            startDate = startDate, endDate = endDate)
phosphorus_data <- readNWISqw(siteNumbers = sites, parameterCd = pcodes_phosphorus,
                              startDate = startDate, endDate = endDate)

# download precip data
precip_stencil <- webgeom(paste0('HUC8::', paste(huc8s, collapse=",")))
precip_knife <- webprocess() # accept defaults for weighted average
all_webdata <- query("webdata")
precip_fabric <- webdata(all_webdata["United States Stage IV Quantitative Precipitation Archive"])
variables(precip_fabric) <- query(precip_fabric, 'variables')
times(precip_fabric) <- c(startDate, endDate)
```

Executing geojob:

```{r execute-job-off, addToggle="ON", eval=FALSE}
precip_geojob <- geoknife(precip_stencil, precip_fabric, precip_knife)
wait(precip_geojob, sleep.time = 10) # add `wait` when running scripts
precip_data <- result(precip_geojob)
```

```{r read-intermediate, echo=FALSE}
geoknife_list <- readRDS("precip_geoknife.RDS")
precip_geojob <- geoknife_list$geojob
precip_data <- geoknife_list$data
```

Creating plots:

```{r sites-data-plot-continued, addToggle="ON"}
precip_data_long <- gather(precip_data, huc8, precip, 
                           -which(!names(precip_data) %in% huc8s))

for(i in sites){
  huc_site_i <- filter(sb_sites_info, site_no == i)$huc_cd # corresponding HUC8

  precip_site_i <- filter(precip_data_long, huc8 == huc_site_i)
  nitrogen_site_i <- filter(nitrogen_data, site_no == i)
  phosphorus_site_i <- filter(phosphorus_data, site_no == i)
  
  layout(matrix(1:3, nrow=3))
  plot(precip_site_i$DateTime, precip_site_i$precip,
       col="red", pch=20, xlab = "Time", ylab = "Precip accumulation, in",
       main = paste("Site", i))
  plot(nitrogen_site_i$sample_dt, nitrogen_site_i$result_va, 
       col="green", pch=20, xlab = "Time", ylab = "Nitrogren concentration, mg/l")
  plot(phosphorus_site_i$sample_dt, phosphorus_site_i$result_va,
       col="blue", pch=20, xlab = "Time", ylab = "Phosphorus concentration, mg/l")
}
```

The challenge with this application was to provide summaries of precipitation, nitrogen, and phosphorus data for a specific set of sites provided by a cooperator. The challenge was to automate the entire workflow, so the final step is to publish our results to ScienceBase. It would make most since to push the results to the same ScienceBase item that the cooperator provided for sites, but since this is an exercise and others will be completing it, we will save the results to a personal SB item. 

Therefore, the first step is to create a folder under your user to save the results. Title this new item "usgs-pkgs-application-results". Visit the [sbtools modify lesson](/usgs-pkgs/sbtools-modify) to remind yourself how to do this. Try it on your own before expanding the solution code!

```{r create-new-item, addToggle="ON"}
# automatically created under the authenticated user
sb_results_item <- item_create(title = "usgs-pkgs-application-results")

# you would only create the item once, then you could just use its id moving forward
sb_results_id <- sb_results_item$id
```

Next save the graphics as PNG files and upload to ScienceBase (add this code to the loop where the plots are rendered). 

```{r automate-plots-publish, addToggle="ON", echo=FALSE}
site_fnames <- paste0("timeseries_", sites, ".png")

for(i in seq_along(sites)){
  site_i <- sites[i]
  huc_site_i <- filter(sb_sites_info, site_no == site_i)$huc_cd # corresponding HUC8

  precip_site_i <- filter(precip_data_long, huc8 == huc_site_i)
  nitrogen_site_i <- filter(nitrogen_data, site_no == site_i)
  phosphorus_site_i <- filter(phosphorus_data, site_no == site_i)
  
  png(filename = site_fnames[i], width=8, height=5, units="in", res=100)
  
  layout(matrix(1:3, nrow=3))
  plot(precip_site_i$DateTime, precip_site_i$precip,
       col="red", pch=20, xlab = "Time", ylab = "Precip accumulation, in",
       main = paste("Site", site_i))
  plot(nitrogen_site_i$sample_dt, nitrogen_site_i$result_va, 
       col="green", pch=20, xlab = "Time", ylab = "Nitrogren concentration, mg/l")
  plot(phosphorus_site_i$sample_dt, phosphorus_site_i$result_va,
       col="blue", pch=20, xlab = "Time", ylab = "Phosphorus concentration, mg/l")
  
  dev.off()
}

updated_item <- item_append_files(sb_results_id, files = site_fnames)
file.remove(site_fnames) # now that it's online, remove local copy
```

Check to see if the files were successfully uploaded to SB:

```{r verify-upload}
sb_fnames <- item_list_files(sb_results_id)
all(site_fnames %in% sb_fnames$fname)
```

You should now have a complete, modular workflow - from data retrieval to processing to visualizing, and finally to publishing and sharing. See the completed workflow below.

```{r complete-workflow, addToggle="ON", eval=FALSE}
library(sbtools)
library(dataRetrieval)
library(geoknife)
library(tidyr)
library(dplyr)

# identify site id and query for files
sb_site_id <- "59848b35e4b0e2f5d46717d1"
avail_files <- item_list_files(sb_site_id)

# use appropriate reader to get file (tab delimited) into R & get site numbers
sb_sites_df <- read.table(avail_files$url[1], sep="\t", header=TRUE,
                          colClasses = "character", stringsAsFactors = FALSE)
sites <- sb_sites_df$site_number

# get HUC 8 codes for precip data
sb_sites_info <- readNWISsite(sites)
huc8s <- sb_sites_info$huc_cd

# define period
startDate <- "2015-10-01"
endDate <- "2016-09-30"

# download nutrient data
pcodes_nitrogen <- c("00613", "00618", "00631")
pcodes_phosphorus <- c("00665")
nitrogen_data <- readNWISqw(siteNumbers = sites, parameterCd = pcodes_nitrogen,
                            startDate = startDate, endDate = endDate)
phosphorus_data <- readNWISqw(siteNumbers = sites, parameterCd = pcodes_phosphorus,
                              startDate = startDate, endDate = endDate)

# download precip data
precip_stencil <- webgeom(paste0('HUC8::', paste(huc8s, collapse=",")))
precip_knife <- webprocess() # accept defaults for weighted average
all_webdata <- query("webdata")
precip_fabric <- webdata(all_webdata["United States Stage IV Quantitative Precipitation Archive"])
variables(precip_fabric) <- query(precip_fabric, 'variables')
times(precip_fabric) <- c(startDate, endDate)
precip_geojob <- geoknife(precip_stencil, precip_fabric, precip_knife)
wait(precip_geojob, sleep.time = 10) # add `wait` when running scripts
precip_data <- result(precip_geojob)

precip_data_long <- gather(precip_data, huc8, precip, 
                           -which(!names(precip_data) %in% huc8s))

# Create and save plots
site_fnames <- paste0("timeseries_", sites, ".png")

for(i in seq_along(sites)){
  site_i <- sites[i]
  huc_site_i <- filter(sb_sites_info, site_no == site_i)$huc_cd # corresponding HUC8

  precip_site_i <- filter(precip_data_long, huc8 == huc_site_i)
  nitrogen_site_i <- filter(nitrogen_data, site_no == site_i)
  phosphorus_site_i <- filter(phosphorus_data, site_no == site_i)
  
  png(filename = site_fnames[i], width=8, height=5, units="in", res=100)
  
  layout(matrix(1:3, nrow=3))
  plot(precip_site_i$DateTime, precip_site_i$precip,
       col="red", pch=20, xlab = "Time", ylab = "Precip accumulation, in",
       main = paste("Site", site_i))
  plot(nitrogen_site_i$sample_dt, nitrogen_site_i$result_va, 
       col="green", pch=20, xlab = "Time", ylab = "Nitrogren concentration, mg/l")
  plot(phosphorus_site_i$sample_dt, phosphorus_site_i$result_va,
       col="blue", pch=20, xlab = "Time", ylab = "Phosphorus concentration, mg/l")
  
  dev.off()
}

updated_item <- item_append_files(sb_results_id, files = site_fnames)
file.remove(site_fnames) # now that it's online, remove local copy
```

```{r sbtools-reset-state, echo=FALSE}
item_rm(sb_results_item, recursive = TRUE)
```
